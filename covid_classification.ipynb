{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import callbacks as call\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
    "from keras.layers.core import Reshape\n",
    "\n",
    "from tensorflow import keras\n",
    "layers = keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TensorFlow Version number:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Running TensorFlow Version number: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./owid-covid-data.csv\")\n",
    "\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSize = int(len(data) * 0.8)     #80% data set for training\n",
    "\n",
    "#Training features\n",
    "trainData = data[1:trainSize][:]\n",
    "trainData.pop('total_deaths')         # The label for the model\n",
    "\n",
    "trainData.pop('iso_code')             # The values are removed from the data\n",
    "trainData.pop('continent')            # because our data should not be biased based on location\n",
    "trainData.pop('location')             # or by date, only by medical statistics\n",
    "trainData.pop('date')\n",
    "trainData.pop('tests_units')\n",
    "trainData = tf.convert_to_tensor(np.asarray(trainData).astype(np.float32), dtype=tf.float32)\n",
    "\n",
    "#Training labels\n",
    "trainLabels = data[1:trainSize]['total_deaths']\n",
    "trainLabels = tf.convert_to_tensor(np.asarray(trainLabels).astype(np.float32), dtype=tf.float32)\n",
    "\n",
    "#Testing Features\n",
    "testData = data[1+trainSize:][:]\n",
    "testData.pop('total_deaths')          # The label for the model\n",
    "\n",
    "testData.pop('iso_code')             # The values are removed from the data\n",
    "testData.pop('continent')            # because our data should not be biased based on location\n",
    "testData.pop('location')             # or by date, only by medical statistics\n",
    "testData.pop('date')\n",
    "testData.pop('tests_units')\n",
    "testData = tf.convert_to_tensor(np.asarray(testData).astype(np.float32), dtype=tf.float32)\n",
    "\n",
    "#Testing labels\n",
    "testLabels = data[1+trainSize:]['total_deaths']\n",
    "testLabels = tf.convert_to_tensor(np.asarray(testLabels).astype(np.float32), dtype=tf.float32)\n",
    "\n",
    "#trainData   = trainData.numpy()\n",
    "#trainLabels = trainLabels.numpy()\n",
    "#testData    = testData.numpy()\n",
    "#testLabels  = testLabels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (these are NumPy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26066\n",
      "28\n",
      "26066\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Scalar tensor has no `len()`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-8407232ac5c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#print(tf.reshape(trainData, shape=(1,trainData.shape)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Users\\Asmod\\anaconda3\\envs\\Tensorflow-GPU\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;34m\"\"\"Returns the length of the first dimension in the Tensor.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Scalar tensor has no `len()`\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Scalar tensor has no `len()`"
     ]
    }
   ],
   "source": [
    "#print(trainData)\n",
    "#print(type(trainData))\n",
    "print(len(trainData))\n",
    "print(len(trainData[0]))\n",
    "#print(trainData.shape)\n",
    "#print(tf.reshape(trainData, shape=(1,trainData.shape)))\n",
    "#print(len(trainLabels))\n",
    "#print(len(trainLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential() # Create the model\n",
    "\n",
    "#x = Reshape((len(trainData),len(trainData[0])))(trainData)\n",
    "x=(len(trainData),len(trainData[0]))\n",
    "model.add(Conv1D(input_shape=x,filters=64,kernel_size=3,padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv1D(filters=64,kernel_size=3,padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2,strides=None))\n",
    "# First 2 convolution layers with a maxpool of 2x2 and stride of 2x2\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2,strides=None))\n",
    "# second 2 convoltion layers with a maxpool of 2x2 and stride of 2x2\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2,strides=None))\n",
    "# third layer of 3 convolution layers with a maxpool of 2x2 and stride of 2x2\n",
    "\n",
    "model.add(Conv1D(filters=512, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv1D(filters=512, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv1D(filters=512, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2,strides=None))\n",
    "# fourth layer of 3 convolution layers with a maxpool of 2x2 and stride of 2x2\n",
    "\n",
    "model.add(Conv1D(filters=512, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv1D(filters=512, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv1D(filters=512, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2,strides=None))\n",
    "# fifth layer of 3 convolution layers with a maxpool of 2x2 and stride of 2x2\n",
    "\n",
    "#Structure detailed by Karen Simonyan and Andrew Zisserman in\n",
    "#VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION\n",
    "# But modified with a reduced dimenion for 2 dimensional data and not images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add(Dropout(0.2))\n",
    "#model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=512,activation=\"relu\"))\n",
    "# First fully connected newtork of 4096 units\n",
    "\n",
    "model.add(Dense(units=512,activation=\"relu\"))\n",
    "# Second fully connected newtork of 4096 units\n",
    "\n",
    "model.add(Dense(units=256,activation=\"relu\"))\n",
    "# Third fully connected newtork of 1000 units\n",
    "\n",
    "#model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "# The soft-max output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#opt = Adam(lr=0.01)\n",
    "\n",
    "#model.compile(optimizer=opt, loss=tensorflow.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "#model.compile(loss='binary_crossentropy', \n",
    "#              optimizer='rmsprop', \n",
    "#              metrics=['accuracy']) \n",
    "sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_39 (Conv1D)           (None, 26066, 64)         5440      \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 26066, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 13033, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 13033, 128)        24704     \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 13033, 128)        49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 6516, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 6516, 256)         98560     \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 6516, 256)         196864    \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 6516, 256)         196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 3258, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 3258, 512)         393728    \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 3258, 512)         786944    \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 3258, 512)         786944    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 1629, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1629, 512)         786944    \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 1629, 512)         786944    \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 1629, 512)         786944    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 814, 512)          0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 814, 512)          262656    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 814, 512)          262656    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 814, 256)          131328    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 814, 1)            257       \n",
      "=================================================================\n",
      "Total params: 5,569,409\n",
      "Trainable params: 5,569,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'tuple'> containing values of types {\"<class 'int'>\", \"<class 'NoneType'>\"}), (<class 'tuple'> containing values of types {\"<class 'int'>\", \"<class 'NoneType'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-a9dba9180516>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m history = model.fit(\n\u001b[0;32m     21\u001b[0m                     \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreduceLoss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                     )\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Users\\Asmod\\anaconda3\\envs\\Tensorflow-GPU\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mF:\\Users\\Asmod\\anaconda3\\envs\\Tensorflow-GPU\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Users\\Asmod\\anaconda3\\envs\\Tensorflow-GPU\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    531\u001b[0m                      'at same time.')\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m   \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m   \u001b[1;31m# Handle validation_split, we want to split the data and get the training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Users\\Asmod\\anaconda3\\envs\\Tensorflow-GPU\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m         \"input: {}, {}\".format(\n\u001b[1;32m--> 998\u001b[1;33m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[0;32m    999\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'tuple'> containing values of types {\"<class 'int'>\", \"<class 'NoneType'>\"}), (<class 'tuple'> containing values of types {\"<class 'int'>\", \"<class 'NoneType'>\"})"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "EPOCHS = 1000\n",
    "training_samples = 4000\n",
    "validation_samples = 600\n",
    "\n",
    "snapshot_name = \"Covid-19_snapshot_\"\n",
    "checkpoint = ModelCheckpoint(snapshot_name+\".{epoch:02d}-{val_loss:.2f}.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduceLoss = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "'''\n",
    "X_new = InputLayer(input_shape=(None, len(trainData),len(trainData[0])), input_tensor=trainData)\n",
    "Y_new = InputLayer(input_shape=(None, len(trainLabels),len(trainLabels[0])), input_tensor=trainLabels)\n",
    "\n",
    "history = model.fit(\n",
    "                    X_new,Y_new, epochs=EPOCHS, validation_split=0.2, verbose=0,\n",
    "                    callbacks=[checkpoint,reduceLoss]\n",
    "                    )\n",
    "'''\n",
    "history = model.fit(\n",
    "                    (len(trainData),len(trainData[0])), (len(trainLabels),1), epochs=EPOCHS, validation_split=0.2, verbose=0,\n",
    "                    callbacks=[checkpoint,reduceLoss]\n",
    "                    )\n",
    "\n",
    "train_generator = generator(trainData, batch_size = 32)\n",
    "validation_generator = generator(testData, batch_size = 32)\n",
    "\n",
    "'''history = model.fit_generator(train_generator, \n",
    "                           steps_per_epoch=training_samples,\n",
    "                           validation_data=validation_generator, \n",
    "                           validation_steps=validation_samples,\n",
    "                           epochs=EPOCHS,\n",
    "                           callbacks=[checkpoint,reduceLoss]\n",
    "                          )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-GPU",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
