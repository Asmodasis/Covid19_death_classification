{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "from tensorflow.keras import callbacks as call\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
    "from keras.layers.core import Reshape\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "layers = keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TensorFlow Version number:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Running TensorFlow Version number: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./owid-covid-data.csv\")\n",
    "\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing: NAN data is problematic for processing, replace all NAN with the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntotal_cases_average                      = data[:][\"total_cases\"]                           .mean()\\nnew_cases_average                        = data[:][\"new_cases\"]                             .mean()\\ntotal_deaths_average                     = data[:][\"total_deaths\"]                          .mean()\\nnew_deaths_average                       = data[:][\"new_deaths\"]                            .mean()\\ntotal_cases_per_million_average          = data[:][\"total_cases_per_million\"]               .mean()\\nnew_cases_per_million_average            = data[:][\"new_cases_per_million\"]                 .mean()\\ntotal_deaths_per_million_average         = data[:][\"total_deaths_per_million\"]              .mean()\\nnew_deaths_per_million_average           = data[:][\"new_deaths_per_million\"]                .mean()\\nnew_tests_average                        = data[:][\"new_tests\"]                             .mean()\\ntotal_tests_average                      = data[:][\"total_tests\"]                           .mean()\\ntotal_tests_per_thousand_average         = data[:][\"total_tests_per_thousand\"]              .mean()\\nnew_tests_per_thousand_average           = data[:][\"new_tests_per_thousand\"]                .mean()\\nnew_tests_smoothed_average               = data[:][\"new_tests_smoothed\"]                    .mean()\\nnew_tests_smoothed_per_thousand_average  = data[:][\"new_tests_smoothed_per_thousand\"]       .mean()\\n#tests_units_average                     = data[:][\"tests_units\"]                           .mean()\\nstringency_index_average                 = data[:][\"stringency_index\"]                      .mean()\\npopulation_average                       = data[:][\"population\"]                            .mean()\\npopulation_density_average               = data[:][\"population_density\"]                    .mean()\\nmedian_age_average                       = data[:][\"median_age\"]                            .mean()\\naged_65_older_average                    = data[:][\"aged_65_older\"]                         .mean()\\naged_70_older_average                    = data[:][\"aged_70_older\"]                         .mean()\\ngdp_per_capita_average                   = data[:][\"gdp_per_capita\"]                        .mean()\\nextreme_poverty_average                  = data[:][\"extreme_poverty\"]                       .mean()\\ncardiovasc_death_rate_average            = data[:][\"cardiovasc_death_rate\"]                 .mean()\\ndiabetes_prevalence_average              = data[:][\"diabetes_prevalence\"]                   .mean()\\nfemale_smokers_average                   = data[:][\"female_smokers\"]                        .mean()\\nmale_smokers_average                     = data[:][\"male_smokers\"]                          .mean()\\nhandwashing_facilities_average           = data[:][\"handwashing_facilities\"]                .mean()\\nhospital_beds_per_thousand_average       = data[:][\"hospital_beds_per_thousand\"]            .mean()\\nlife_expectancy_average                  = data[:][\"life_expectancy\"]                       .mean()\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "total_cases_average                      = data[:][\"total_cases\"]                           .mean()\n",
    "new_cases_average                        = data[:][\"new_cases\"]                             .mean()\n",
    "total_deaths_average                     = data[:][\"total_deaths\"]                          .mean()\n",
    "new_deaths_average                       = data[:][\"new_deaths\"]                            .mean()\n",
    "total_cases_per_million_average          = data[:][\"total_cases_per_million\"]               .mean()\n",
    "new_cases_per_million_average            = data[:][\"new_cases_per_million\"]                 .mean()\n",
    "total_deaths_per_million_average         = data[:][\"total_deaths_per_million\"]              .mean()\n",
    "new_deaths_per_million_average           = data[:][\"new_deaths_per_million\"]                .mean()\n",
    "new_tests_average                        = data[:][\"new_tests\"]                             .mean()\n",
    "total_tests_average                      = data[:][\"total_tests\"]                           .mean()\n",
    "total_tests_per_thousand_average         = data[:][\"total_tests_per_thousand\"]              .mean()\n",
    "new_tests_per_thousand_average           = data[:][\"new_tests_per_thousand\"]                .mean()\n",
    "new_tests_smoothed_average               = data[:][\"new_tests_smoothed\"]                    .mean()\n",
    "new_tests_smoothed_per_thousand_average  = data[:][\"new_tests_smoothed_per_thousand\"]       .mean()\n",
    "#tests_units_average                     = data[:][\"tests_units\"]                           .mean()\n",
    "stringency_index_average                 = data[:][\"stringency_index\"]                      .mean()\n",
    "population_average                       = data[:][\"population\"]                            .mean()\n",
    "population_density_average               = data[:][\"population_density\"]                    .mean()\n",
    "median_age_average                       = data[:][\"median_age\"]                            .mean()\n",
    "aged_65_older_average                    = data[:][\"aged_65_older\"]                         .mean()\n",
    "aged_70_older_average                    = data[:][\"aged_70_older\"]                         .mean()\n",
    "gdp_per_capita_average                   = data[:][\"gdp_per_capita\"]                        .mean()\n",
    "extreme_poverty_average                  = data[:][\"extreme_poverty\"]                       .mean()\n",
    "cardiovasc_death_rate_average            = data[:][\"cardiovasc_death_rate\"]                 .mean()\n",
    "diabetes_prevalence_average              = data[:][\"diabetes_prevalence\"]                   .mean()\n",
    "female_smokers_average                   = data[:][\"female_smokers\"]                        .mean()\n",
    "male_smokers_average                     = data[:][\"male_smokers\"]                          .mean()\n",
    "handwashing_facilities_average           = data[:][\"handwashing_facilities\"]                .mean()\n",
    "hospital_beds_per_thousand_average       = data[:][\"hospital_beds_per_thousand\"]            .mean()\n",
    "life_expectancy_average                  = data[:][\"life_expectancy\"]                       .mean()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:][\"total_cases\"]                     =data[:][\"total_cases\"]                      .fillna(data[:][\"total_cases\"]                           .mean())\n",
    "data[:][\"new_cases\"]                       =data[:][\"new_cases\"]                        .fillna(data[:][\"new_cases\"]                             .mean())\n",
    "data[:][\"total_deaths\"]                    =data[:][\"total_deaths\"]                     .fillna(data[:][\"total_deaths\"]                          .mean())\n",
    "data[:][\"new_deaths\"]                      =data[:][\"new_deaths\"]                       .fillna(data[:][\"new_deaths\"]                            .mean())\n",
    "data[:][\"total_cases_per_million\"]         =data[:][\"total_cases_per_million\"]          .fillna(data[:][\"total_cases_per_million\"]               .mean())\n",
    "data[:][\"new_cases_per_million\"]           =data[:][\"new_cases_per_million\"]            .fillna(data[:][\"new_cases_per_million\"]                 .mean())\n",
    "data[:][\"total_deaths_per_million\"]        =data[:][\"total_deaths_per_million\"]         .fillna(data[:][\"total_deaths_per_million\"]              .mean())\n",
    "data[:][\"new_deaths_per_million\"]          =data[:][\"new_deaths_per_million\"]           .fillna(data[:][\"new_deaths_per_million\"]                .mean())\n",
    "data[:][\"new_tests\"]                       =data[:][\"new_tests\"]                        .fillna(data[:][\"new_tests\"]                             .mean())\n",
    "data[:][\"total_tests\"]                     =data[:][\"total_tests\"]                      .fillna(data[:][\"total_tests\"]                           .mean())\n",
    "data[:][\"total_tests_per_thousand\"]        =data[:][\"total_tests_per_thousand\"]         .fillna(data[:][\"total_tests_per_thousand\"]              .mean())\n",
    "data[:][\"new_tests_per_thousand\"]          =data[:][\"new_tests_per_thousand\"]           .fillna(data[:][\"new_tests_per_thousand\"]                .mean())\n",
    "data[:][\"new_tests_smoothed\"]              =data[:][\"new_tests_smoothed\"]               .fillna(data[:][\"new_tests_smoothed\"]                    .mean())\n",
    "data[:][\"new_tests_smoothed_per_thousand\"] =data[:][\"new_tests_smoothed_per_thousand\"]  .fillna(data[:][\"new_tests_smoothed_per_thousand\"]       .mean())\n",
    "#data[:][\"tests_units\"]                    =data[:][\"tests_units\"]                      .fillna(#data[:][\"tests_units\"]                           .mean())\n",
    "data[:][\"stringency_index\"]                =data[:][\"stringency_index\"]                 .fillna(data[:][\"stringency_index\"]                      .mean())\n",
    "data[:][\"population\"]                      =data[:][\"population\"]                       .fillna(data[:][\"population\"]                            .mean())\n",
    "data[:][\"population_density\"]              =data[:][\"population_density\"]               .fillna(data[:][\"population_density\"]                    .mean())\n",
    "data[:][\"median_age\"]                      =data[:][\"median_age\"]                       .fillna(data[:][\"median_age\"]                            .mean())\n",
    "data[:][\"aged_65_older\"]                   =data[:][\"aged_65_older\"]                    .fillna(data[:][\"aged_65_older\"]                         .mean())\n",
    "data[:][\"aged_70_older\"]                   =data[:][\"aged_70_older\"]                    .fillna(data[:][\"aged_70_older\"]                         .mean())\n",
    "data[:][\"gdp_per_capita\"]                  =data[:][\"gdp_per_capita\"]                   .fillna(data[:][\"gdp_per_capita\"]                        .mean())\n",
    "data[:][\"extreme_poverty\"]                 =data[:][\"extreme_poverty\"]                  .fillna(data[:][\"extreme_poverty\"]                       .mean())\n",
    "data[:][\"cardiovasc_death_rate\"]           =data[:][\"cardiovasc_death_rate\"]            .fillna(data[:][\"cardiovasc_death_rate\"]                 .mean())\n",
    "data[:][\"diabetes_prevalence\"]             =data[:][\"diabetes_prevalence\"]              .fillna(data[:][\"diabetes_prevalence\"]                   .mean())\n",
    "data[:][\"female_smokers\"]                  =data[:][\"female_smokers\"]                   .fillna(data[:][\"female_smokers\"]                        .mean())\n",
    "data[:][\"male_smokers\"]                    =data[:][\"male_smokers\"]                     .fillna(data[:][\"male_smokers\"]                          .mean())\n",
    "data[:][\"handwashing_facilities\"]          =data[:][\"handwashing_facilities\"]           .fillna(data[:][\"handwashing_facilities\"]                .mean())\n",
    "data[:][\"hospital_beds_per_thousand\"]      =data[:][\"hospital_beds_per_thousand\"]       .fillna(data[:][\"hospital_beds_per_thousand\"]            .mean())\n",
    "data[:][\"life_expectancy\"]                 =data[:][\"life_expectancy\"]                  .fillna(data[:][\"life_expectancy\"]                       .mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        0.500000\n",
       "2        0.500000\n",
       "3        0.500000\n",
       "4        0.500000\n",
       "5        0.500000\n",
       "           ...   \n",
       "26062    3.150124\n",
       "26063    3.150124\n",
       "26064    3.150124\n",
       "26065    3.150124\n",
       "26066    3.150124\n",
       "Name: hospital_beds_per_thousand, Length: 26066, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData = data[1:trainSize][:]\n",
    "testData = data[1+trainSize:][:]\n",
    "\n",
    "testData.pop('total_cases_per_million')            \n",
    "testData.pop('new_cases_per_million')          \n",
    "testData.pop('total_deaths_per_million')      \n",
    "testData.pop('new_deaths_per_million')\n",
    "testData.pop('new_tests')\n",
    "testData.pop('total_tests_per_thousand')            \n",
    "testData.pop('new_tests_per_thousand')          \n",
    "testData.pop('new_tests_smoothed')      \n",
    "testData.pop('new_tests_smoothed_per_thousand')\n",
    "testData.pop('stringency_index')\n",
    "testData.pop('median_age')            \n",
    "testData.pop('aged_70_older')          \n",
    "testData.pop('gdp_per_capita')      \n",
    "testData.pop('extreme_poverty')\n",
    "testData.pop('cardiovasc_death_rate')\n",
    "testData.pop('diabetes_prevalence')\n",
    "testData.pop('female_smokers')\n",
    "testData.pop('male_smokers')\n",
    "testData.pop('handwashing_facilities')\n",
    "testData.pop('hospital_beds_per_thousand')\n",
    "\n",
    "trainData.pop('total_cases_per_million')            \n",
    "trainData.pop('new_cases_per_million')          \n",
    "trainData.pop('total_deaths_per_million')      \n",
    "trainData.pop('new_deaths_per_million')\n",
    "trainData.pop('new_tests')\n",
    "trainData.pop('total_tests_per_thousand')            \n",
    "trainData.pop('new_tests_per_thousand')          \n",
    "trainData.pop('new_tests_smoothed')      \n",
    "trainData.pop('new_tests_smoothed_per_thousand')\n",
    "trainData.pop('stringency_index')\n",
    "trainData.pop('median_age')            \n",
    "trainData.pop('aged_70_older')          \n",
    "trainData.pop('gdp_per_capita')      \n",
    "trainData.pop('extreme_poverty')\n",
    "trainData.pop('cardiovasc_death_rate')\n",
    "trainData.pop('diabetes_prevalence')\n",
    "trainData.pop('female_smokers')\n",
    "trainData.pop('male_smokers')\n",
    "trainData.pop('handwashing_facilities')\n",
    "trainData.pop('hospital_beds_per_thousand')\n",
    "\n",
    "#testData.pop('')\n",
    "\n",
    "#testData = tf.convert_to_tensor(np.asarray(testData).astype(np.float32), dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSize = int(len(data) * 0.8)     #80% data set for training\n",
    "\n",
    "#Training features\n",
    "#trainData = data[1:trainSize][:]\n",
    "trainData.pop('total_deaths')         # The label for the model\n",
    "\n",
    "trainData.pop('iso_code')             # The values are removed from the data\n",
    "trainData.pop('continent')            # because our data should not be biased based on location\n",
    "trainData.pop('location')             # or by date, only by medical statistics\n",
    "trainData.pop('date')\n",
    "trainData.pop('tests_units')\n",
    "trainData = tf.convert_to_tensor(np.asarray(trainData).astype(np.float32), dtype=tf.float32)\n",
    "\n",
    "#Training labels\n",
    "tempTrainLabels = data[1:trainSize]['total_deaths']\n",
    "tempTrainLabels = tf.convert_to_tensor(np.asarray(tempTrainLabels).astype(np.float32), dtype=tf.float32)\n",
    "\n",
    "#Testing Features\n",
    "#testData = data[1+trainSize:][:]\n",
    "testData.pop('total_deaths')          # The label for the model\n",
    "\n",
    "testData.pop('iso_code')             # The values are removed from the data\n",
    "testData.pop('continent')            # because our data should not be biased based on location\n",
    "testData.pop('location')             # or by date, only by medical statistics\n",
    "testData.pop('date')\n",
    "testData.pop('tests_units')\n",
    "testData = tf.convert_to_tensor(np.asarray(testData).astype(np.float32), dtype=tf.float32)\n",
    "\n",
    "#Testing labels\n",
    "tempTestLabels = data[1+trainSize:]['total_deaths']\n",
    "tempTestLabels = tf.convert_to_tensor(np.asarray(tempTestLabels).astype(np.float32), dtype=tf.float32)\n",
    "\n",
    "trainData        = trainData.numpy()\n",
    "tempTrainLabels  = tempTrainLabels.numpy()\n",
    "testData         = testData.numpy()\n",
    "tempTestLabels   = tempTestLabels.numpy()\n",
    "\n",
    "\n",
    "trainLabels   = tempTrainLabels\n",
    "testLabels    = tempTestLabels\n",
    "#trainLabels  = tf.keras.utils.to_categorical(tempTrainLabels, num_classes=1)\n",
    "\n",
    "#testLabels   = tf.keras.utils.to_categorical(tempTestLabels, num_classes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(trainLabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data shapes\n",
      "(8,)\n",
      "(26066, 8)\n",
      "()\n",
      "(26066,)\n",
      "Testing Data shapes\n",
      "(8,)\n",
      "(6516, 8)\n",
      "()\n",
      "(6516,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data shapes\")\n",
    "print((trainData[0]).shape)\n",
    "print((trainData).shape)\n",
    "print((trainLabels[0]).shape)\n",
    "print((trainLabels).shape)\n",
    "\n",
    "print(\"Testing Data shapes\")\n",
    "print((testData[0]).shape)\n",
    "print((testData).shape)\n",
    "print((testLabels[0]).shape)\n",
    "print((testLabels).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Users\\Asmod\\anaconda3\\envs\\Tensorflow-GPU\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000000.0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(preprocessing.normalize(trainData), preprocessing.LabelEncoder().fit_transform(trainLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = Model()\n",
    "\n",
    "model.add(Dense(1, \n",
    "                activation='softmax',\n",
    "                kernel_regularizer=L1L2(l1=0.0, l2=0.1), \n",
    "                input_shape=(28,)\n",
    "               ),\n",
    "            #)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "                optimizer='sgd',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "                ),\n",
    "model.fit(trainData, trainLabels, epochs=50))\n",
    "'''\n",
    "\n",
    "inputs = keras.Input(shape=(8,))\n",
    "\n",
    "dense = layers.Dense(32, activation=\"sigmoid\")\n",
    "x = dense(inputs)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"covid_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"covid_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 321\n",
      "Trainable params: 321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26066 samples, validate on 6516 samples\n",
      "Epoch 1/20\n",
      "26066/26066 [==============================] - 1s 41us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 2/20\n",
      "26066/26066 [==============================] - 1s 30us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 3/20\n",
      "26066/26066 [==============================] - 1s 29us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 4/20\n",
      "26066/26066 [==============================] - 1s 30us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 5/20\n",
      "26066/26066 [==============================] - 1s 30us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 6/20\n",
      "26066/26066 [==============================] - 1s 30us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 7/20\n",
      "26066/26066 [==============================] - 1s 29us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 8/20\n",
      "26066/26066 [==============================] - 1s 29us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 9/20\n",
      "26066/26066 [==============================] - 1s 30us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 10/20\n",
      "26066/26066 [==============================] - 1s 29us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 11/20\n",
      "26066/26066 [==============================] - 1s 29us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 12/20\n",
      "26066/26066 [==============================] - 1s 30us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 13/20\n",
      "26066/26066 [==============================] - 1s 30us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 14/20\n",
      "26066/26066 [==============================] - 1s 29us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 15/20\n",
      "26066/26066 [==============================] - 1s 29us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 16/20\n",
      "26066/26066 [==============================] - 1s 30us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 17/20\n",
      "26066/26066 [==============================] - 1s 30us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 18/20\n",
      "26066/26066 [==============================] - 1s 31us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 19/20\n",
      "26066/26066 [==============================] - 1s 31us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n",
      "Epoch 20/20\n",
      "26066/26066 [==============================] - 1s 31us/sample - loss: 14.0517 - accuracy: 0.3144 - val_loss: 13.0151 - val_accuracy: 0.2715\n"
     ]
    }
   ],
   "source": [
    "sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(\n",
    "    loss = \"mean_squared_logarithmic_error\", optimizer = sgd,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(preprocessing.normalize(trainData), \n",
    "                    preprocessing.LabelEncoder().fit_transform(trainLabels), \n",
    "                    batch_size=32, \n",
    "                    epochs=20, \n",
    "                    validation_data=(\n",
    "                            (preprocessing.normalize(testData), preprocessing.LabelEncoder().fit_transform(testLabels)\n",
    "                            ))\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-GPU",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
